{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# FDU PRML 2023 Fall Assignment 2.1\n",
    "\n",
    "Name: <your name> 王炜\n",
    "Student ID: <your student id> 21300190011\n",
    "\n",
    "<font color='red'>**Deadline: 2023-11-20 23:59**</font>\n",
    "<font color='red'>**Overall score weight: 70/100**</font>\n",
    "\n",
    "In this semester, we are going to complete 3 assignments, each may contain **2-3 parts**. This is the second (and the last) part of the second assignment, in which we will get to implement our own Pytorch-like library.\n",
    "\n",
    "## 1. FDUNN: your toy torch-like deep learning library (40 points)\n",
    "\n",
    "In this assignment, you will fist implement your own torch-like deep learning library with `numpy`, named `fdunn`.\n",
    "\n",
    "PyTorch: [Link](https://pytorch.org/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# setup code, auto reload your .py file\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# numpy\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(233)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# matplotlib\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "You have to impletement several standard deep neural network modules in the `./fdunn` folder:\n",
    "1.   linear/conv/pooling\n",
    "2.   activation\n",
    "3.   loss\n",
    "4.   optim\n",
    "5.   trainer\n",
    "\n",
    "We have written most of the code for you already, and you only need to fill in the most essential parts. We have also prepared several test cases for you to check if your code works correctly.\n",
    "\n",
    "Furthermore, you can also test the accuracy of your code by comparing its output with the output of sk-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from fdunn.modules import base\n",
    "from fdunn.modules import conv\n",
    "from fdunn.modules import linear\n",
    "from fdunn.modules import pooling\n",
    "from fdunn.modules import activation\n",
    "from fdunn.modules import loss\n",
    "from fdunn.optim import base as op_base\n",
    "from fdunn.optim import sgd\n",
    "from fdunn import trainer\n",
    "from fdunn.metrics import f1_score,accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2. Deep Learning with Image/Text Data (20 points)\n",
    "\n",
    "Use your fdunn lib to perform image classification on MNIST or CIFAR10 dataset.\n",
    "\n",
    "- MNIST: http://yann.lecun.com/exdb/mnist/\n",
    "- CIFAR10: https://www.cs.toronto.edu/~kriz/cifar.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The flatten object is for conforming the data to the linear layer, that is, flattening all data belonging to the same figure into a 1D array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class flatten(base.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.params = None\n",
    "        self.grads = None\n",
    "\n",
    "    def forward(self, data: np.ndarray):\n",
    "        \"\"\"data:(N,C,H,W),\n",
    "        output:(N,C*H*W)\"\"\"\n",
    "        self._N, self._C, self._H, self._W = data.shape\n",
    "        return data.reshape(data.shape[0], -1)\n",
    "\n",
    "    def backward(self, output_grad: np.ndarray):\n",
    "        output_grad = np.stack(\n",
    "            [\n",
    "                output_grad[:, i * (self._H * self._W) : (i + 1) * (self._H * self._W)]\n",
    "                for i in range(self._C)\n",
    "            ],\n",
    "            axis=1,\n",
    "        )\n",
    "        output_grad = np.stack(\n",
    "            [\n",
    "                output_grad[:, :, i * self._W : (i + 1) * self._W]\n",
    "                for i in range(self._H)\n",
    "            ],\n",
    "            axis=2,\n",
    "        )\n",
    "        return output_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(base.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.train_batches = []\n",
    "        self.conv1 = conv.Conv2d(\n",
    "            in_channels=3, out_channels=6, kernel_size=4, stride=2, padding=2\n",
    "        )\n",
    "        self.act1 = activation.Sigmoid()\n",
    "        self.pool1 = pooling.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.conv2 = conv.Conv2d(\n",
    "            in_channels=6, out_channels=10, kernel_size=2, stride=1, padding=1\n",
    "        )\n",
    "        self.act2 = activation.Sigmoid()\n",
    "        self.pool2 = pooling.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.flatten = flatten()\n",
    "\n",
    "        _a = self.conv1.kernel_size[0]\n",
    "        _b = self.conv1.stride[0]\n",
    "        _c = self.conv1.padding[0]\n",
    "        _d = self.pool1.kernel_size[0]\n",
    "        _e = self.pool1.stride[0]\n",
    "        _f = self.conv2.kernel_size[0]\n",
    "        _g = self.conv2.stride[0]\n",
    "        _h = self.conv2.padding[0]\n",
    "        _i = self.pool2.kernel_size[0]\n",
    "        _j = self.pool2.stride[0]\n",
    "        _n1 = np.floor((32 + 2 * _c - _a) / _b + 1)\n",
    "        _n2 = np.floor((_n1 - _d) / _e + 1)\n",
    "        _n3 = np.floor((_n2 + 2 * _h - _f) / _g + 1)\n",
    "        _n4 = np.floor((_n3 - _i) / _j + 1)\n",
    "\n",
    "        self.fn = linear.Linear(\n",
    "            in_features=int(self.conv2.out_channels * _n4**2), out_features=10\n",
    "        )\n",
    "\n",
    "        self.layers = [\n",
    "            self.conv1,\n",
    "            self.act1,\n",
    "            self.pool1,\n",
    "            self.conv2,\n",
    "            self.act2,\n",
    "            self.pool2,\n",
    "            self.flatten,\n",
    "            self.fn,\n",
    "        ]\n",
    "\n",
    "        self.conv1.name = \"conv1\"\n",
    "        self.act1.name = \"act1\"\n",
    "        self.pool1.name = \"pool1\"\n",
    "        self.conv2.name = \"conv2\"\n",
    "        self.act2.name = \"act2\"\n",
    "        self.pool2.name = \"pool2\"\n",
    "        self.flatten.name = \"flatten\"\n",
    "        self.fn.name = \"fn\"\n",
    "\n",
    "    def fold(self, data: np.ndarray, datay: list, batch_size: int, valratio: float):\n",
    "        \"\"\"data: (N,3*32*32),datay: (N)\n",
    "        train_batches: [(batch_size,3,32,32),...],\n",
    "        trainlabels:[(batch_size)],\n",
    "        valid:(N*valratio,3,32,32)\n",
    "        validlabels=(N*valratio)\"\"\"\n",
    "        assert (\n",
    "            data.shape[0] % batch_size == 0\n",
    "        ), \"batch_size must be divisible by datasize\"\n",
    "        data1 = np.stack([data[:, :1024], data[:, 1024:2048], data[:, 2048:]], axis=1)\n",
    "        data2 = np.stack(\n",
    "            [data1[:, :, 32 * i : 32 * (i + 1)] for i in range(32)], axis=2\n",
    "        )\n",
    "        self.train_batches.extend(\n",
    "            [\n",
    "                data2[batch_size * i : batch_size * (i + 1)]\n",
    "                for i in range(int(data.shape[0] * (1 - valratio) / batch_size))\n",
    "            ]\n",
    "        )\n",
    "        self.valid_data = data2[\n",
    "            batch_size * int(data.shape[0] * (1 - valratio) / batch_size) :\n",
    "        ]\n",
    "        datay = np.array(datay)\n",
    "        self.trainlabels = [\n",
    "            datay[batch_size * i : batch_size * (i + 1)]\n",
    "            for i in range(int(data.shape[0] * (1 - valratio) / batch_size))\n",
    "        ]\n",
    "        self.validlabels = datay[\n",
    "            batch_size * int(data.shape[0] * (1 - valratio) / batch_size) :\n",
    "        ]\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.act2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fn(x)\n",
    "        return x\n",
    "\n",
    "    def backward(self, grad):\n",
    "        grad = self.fn.backward(grad)\n",
    "        grad = self.flatten.backward(grad)\n",
    "        grad = self.pool2.backward(grad)\n",
    "        grad = self.act2.backward(grad)\n",
    "        grad = self.conv2.backward(grad)\n",
    "        grad = self.pool1.backward(grad)\n",
    "        grad = self.act1.backward(grad)\n",
    "        self.conv1.backward(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "file = \"C:/Assignment2/Assignment2.2/cifar-10-batches-py/data_batch_{}\"\n",
    "\n",
    "\n",
    "def unpickle(file):\n",
    "    import pickle\n",
    "\n",
    "    with open(file, \"rb\") as fo:\n",
    "        dict = pickle.load(fo, encoding=\"bytes\")\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = unpickle(file.format(2))\n",
    "data1 = unpickle(file.format(1))\n",
    "data=np.concatenate((data2[b'data'],data1[b'data']),axis=0)\n",
    "labels=data2[b'labels']+(data1[b'labels'])\n",
    "model = CNN()\n",
    "model.fold(data,labels, batch_size=100, valratio=0.2)\n",
    "optimizer = sgd.SGD(model, lr=0.001)\n",
    "metric = f1_score.F1_score()\n",
    "lossfunction = loss.CrossEntropyLoss(model, reduction=\"sum\")\n",
    "trainor = trainer.Trainer(model, optimizer, metric, lossfunction)\n",
    "trainset = tuple(zip(model.train_batches, model.trainlabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current best score is 0.2487886690203001\n",
      "[Train] epoch: 0/100, loss: 35269.07932768929\n",
      "[Train] epoch: 5/100, loss: 36031.00957723002\n",
      "[Train] epoch: 10/100, loss: 36022.747012376334\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10092\\1154334710.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mnum_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mlog_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0msave_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"C:/Assignment2/Assignment2.2/savemodel/\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m )\n",
      "\u001b[1;32mc:\\Assignment2\\Assignment2.2\\fdunn\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, train_set, dev_set, **kwargs)\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[0mtrn_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m                 \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m                 \u001b[0mtrn_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# return a tensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Assignment2\\Assignment2.2\\fdunn\\modules\\base.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10092\\1516171550.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpool1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 91\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpool2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Assignment2\\Assignment2.2\\fdunn\\modules\\base.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Assignment2\\Assignment2.2\\fdunn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    160\u001b[0m                     \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m                 )\n\u001b[1;32m--> 162\u001b[1;33m                 \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_padded\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    163\u001b[0m             ],\n\u001b[0;32m    164\u001b[0m             \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Assignment2\\Assignment2.2\\fdunn\\modules\\conv.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    160\u001b[0m                     \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m                 )\n\u001b[1;32m--> 162\u001b[1;33m                 \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_padded\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    163\u001b[0m             ],\n\u001b[0;32m    164\u001b[0m             \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mstack\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32mc:\\miniconda_env\\envs\\gnn1\\lib\\site-packages\\numpy\\core\\shape_base.py\u001b[0m in \u001b[0;36mstack\u001b[1;34m(arrays, axis, out)\u001b[0m\n\u001b[0;32m    431\u001b[0m     \u001b[0msl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mslice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0m_nx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    432\u001b[0m     \u001b[0mexpanded_arrays\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msl\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 433\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexpanded_arrays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    434\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    435\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainor.load_model(\"C:/Assignment2/Assignment2.2/savemodel/\")\n",
    "trainor.train(\n",
    "    trainset,\n",
    "    (model.valid_data, model.validlabels),\n",
    "    num_epochs=100,\n",
    "    log_epochs=5,\n",
    "    save_dir=\"C:/Assignment2/Assignment2.2/savemodel/\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The loss is the total loss on all figures so the scale is a bit large. The matrics is uniform weighed average f1score. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Then test it with f1score on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current best score is 0.2487886690203001\n",
      "the f1score on test set is 0.25984329934718414\n"
     ]
    }
   ],
   "source": [
    "testfile = \"C:/Assignment2/Assignment2.2/cifar-10-batches-py/test_batch\"\n",
    "testdata=unpickle(file)\n",
    "trainor.load_model(\"C:/Assignment2/Assignment2.2/savemodel/\")\n",
    "model.fold(testdata[b'data'],testdata[b'labels'],batch_size=10000,valratio=1)\n",
    "score,_=trainor.evaluate((model.valid_data, model.validlabels))\n",
    "print(f'the f1score on test set is {score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### additionally check out its accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testfile = \"C:/Assignment2/Assignment2.2/cifar-10-batches-py/test_batch\"\n",
    "testdata=unpickle(file)\n",
    "model = CNN()\n",
    "optimizer = sgd.SGD(model, lr=0.001)\n",
    "metric = accuracy.Accuracy()\n",
    "lossfunction = loss.CrossEntropyLoss(model, reduction=\"sum\")\n",
    "trainor = trainer.Trainer(model, optimizer, metric, lossfunction)\n",
    "trainor.load_model(\"C:/Assignment2/Assignment2.2/savemodel/\")\n",
    "model.fold(testdata[b\"data\"], testdata[b\"labels\"], batch_size=10000, valratio=1)\n",
    "score,_=trainor.evaluate((model.valid_data, model.validlabels))\n",
    "print(f'the accuracy on test set is {score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the performance is slightly higher than random classification. Given the high total loss on train set it's apparent that the model itself is not strong enough. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3. Advanced Topics (10 points)\n",
    "\n",
    "You can try to implement some advanced topics in deep learning with our fdunn in this section.\n",
    "\n",
    "We will divide the topics into two categories: modules and optimization, with 5 points for each category.\n",
    "\n",
    "### 3.1 Modules\n",
    "\n",
    "Pick one of the following (trending or classic) topics and implement it with fdunn. Try to implement it and design a toy test case to show that your implementation is correct (say we can compare the forward and backward results with the results of corresponding Pytorch modules with the same weight).\n",
    "\n",
    "- Batch/Layer/Group Normalization (just one of them is fine)\n",
    "- A ReLU/GeLU/SiLU activation function (just one of them is fine)\n",
    "- A Gated Linear Unit (GLU)\n",
    "- An RNN cell\n",
    "- Or any other modules you are interested in\n",
    "\n",
    "\n",
    "### 3.2 Optimization\n",
    "\n",
    "Pick one of the following optimization methods and implement it with fdunn.\n",
    "\n",
    "- SGD with L2 regularization. (Try to repeat your experiment in Assignment 1.1 to see the same 'underregularization' to 'overregularization' phenomenon)\n",
    "- Adam (Does it converge faster than SGD in the last section?)\n",
    "- Or any other optimization methods you are interested in (I am far from an expert in optimization, so maybe you can teach me something new here. Say a second order method?)\n",
    "\n",
    "\n",
    "If you are not sure about what to do, We suggest you to implement the ReLU activation function and SGD with L2 regularization. They are foundamental and (relatively) easy to implement.\n",
    "\n",
    "Different methods do not vary in scores. So do not chase fancy methods unless you are quite interested in them.\n",
    "\n",
    "There are many ways to prove your implementation is correct, as long as they are convincing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 ReLU activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ReLU(base.Module):\n",
    "    def __init__(self):\n",
    "        super(ReLU, self).__init__()\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "        self.params = None\n",
    "        self.grads = None\n",
    "\n",
    "    def forward(self, input: np.ndarray):\n",
    "        \"\"\"input:(*,C,H,W),\n",
    "        return:(*,C,H,W)\"\"\"\n",
    "        self.input = input\n",
    "        self.output = self.input\n",
    "        self.output[self.output < 0] = 0\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, output_grad: np.ndarray):\n",
    "        \"\"\"output_grad:(*,C,H,W),\n",
    "        return input_grad:(*,C,H,W)\"\"\"\n",
    "        output_grad[self.input <= 0] = 0\n",
    "        return output_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compare relu to that of torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "same output as torch\n",
      "same grad as torch\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "input_data = torch.randn(3, 4, 5, 6)\n",
    "input_data.requires_grad_()\n",
    "torch_output = F.relu(input_data)\n",
    "output_grad = torch.randn(3, 4, 5, 6)\n",
    "torch_output.backward(output_grad)\n",
    "torch_grad = input_data.grad.detach().numpy()\n",
    "torch_output = torch_output.detach().numpy()\n",
    "input_data = input_data.detach().numpy()\n",
    "output_grad = output_grad.detach().numpy()\n",
    "relu = ReLU()\n",
    "np_output = relu.forward(input_data)\n",
    "np_grad = relu.backward(output_grad)\n",
    "if (np_output == torch_output).all():\n",
    "    print(\"same output as torch\")\n",
    "if (np_grad == torch_grad).all():\n",
    "    print(\"same grad as torch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 SGD with L2 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class L2SGD(op_base.Optimizer):\n",
    "    def __init__(self, model, lr=0.01, lambda_reg=0):\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "        self.lambda_reg = lambda_reg\n",
    "\n",
    "    def step(self):\n",
    "        for layer in self.model.layers:\n",
    "            if isinstance(layer.params, dict):\n",
    "                for key in layer.params.keys():\n",
    "                    layer.params[key] = layer.params[key] - self.lr * (\n",
    "                        layer.grads[key] + self.lambda_reg * layer.params[key]\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### try to repeat the experiment in Assignment1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class linearmodel(base.Module):\n",
    "    def __init__(self,inchannel,outchannel):\n",
    "        super(linearmodel, self).__init__()\n",
    "        self.linear = linear.Linear(inchannel,outchannel,False)\n",
    "        self.layers = [self.linear]\n",
    "        self.linear.name = \"linear\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "    def backward(self, output_grad):\n",
    "        self.linear.backward(output_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE as loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSE(base.Loss):\n",
    "    def __init__(self, model):\n",
    "        self.input = None\n",
    "        self.target = None\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, y_pred: np.ndarray, y_true: np.ndarray):\n",
    "        self.y_pred = y_pred\n",
    "        self.y_true = y_true\n",
    "        loss = ((y_pred - y_true) ** 2).mean()\n",
    "        return loss\n",
    "\n",
    "    def backward(self):\n",
    "        pred_grad = (self.y_pred - self.y_true) * 2 / self.y_pred.size\n",
    "        self.model.backward(pred_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fit function is for training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(\n",
    "    model: linearmodel,\n",
    "    optimize: L2SGD,\n",
    "    loss_fn:MSE,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=100,\n",
    "    showprogress=False,\n",
    "):\n",
    "    loss_ = []\n",
    "    for epoch in range(epochs):\n",
    "        y_pred = model(X_train)\n",
    "        loss_.append(float(loss_fn(input=y_pred,target=y_train)))\n",
    "        loss_fn.backward()\n",
    "        optimize.step()\n",
    "    print(f'loss descend from {loss_[0]} to {loss_[-1]}') \n",
    "    \n",
    "    if showprogress:\n",
    "        plt.plot(loss_)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the following is exactly like what's been done in Assignment1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sin\n",
    "import math\n",
    "from sklearn.preprocessing import PolynomialFeatures as pf\n",
    "\n",
    "def sin(x):\n",
    "    y = np.sin(2 * math.pi * x)\n",
    "    return y\n",
    "\n",
    "\n",
    "def create_toy_data(\n",
    "    func, interval, sample_num, noise=0.0, add_outlier=False, outlier_ratio=0.001\n",
    "):\n",
    "    \"\"\"\n",
    "    generate data with the given function\n",
    "    input:\n",
    "       - func: the input function\n",
    "       - interval: the range of values of x, a tuple (start, end)\n",
    "       - sample_num: number of samples\n",
    "       - noise: the standard deviation of Gaussian noise\n",
    "       - add_outlier: whether to generate outliers\n",
    "       - outlier_ratio: proportion of outliers\n",
    "\n",
    "    output:\n",
    "       - X: samples, shape = [n_samples,1]\n",
    "       - y: labels, shape = [n_samples,1]\n",
    "    \"\"\"\n",
    "\n",
    "    X = np.random.rand(sample_num, 1) * (interval[1] - interval[0]) + interval[0]\n",
    "    y = func(X)\n",
    "\n",
    "    # add Gaussian noise\n",
    "    epsilon = np.random.normal(0, noise, (sample_num, 1))\n",
    "    y = y + epsilon\n",
    "\n",
    "    # add outlier\n",
    "    if add_outlier:\n",
    "        outlier_num = int(sample_num * outlier_ratio)\n",
    "        if outlier_num != 0:\n",
    "            outlier_idx = np.random.randint(sample_num, size=[outlier_num, 1])\n",
    "            y[outlier_idx] = y[outlier_idx] * 5\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "func = sin\n",
    "interval = (0, 1)\n",
    "train_num = 15\n",
    "test_num = 10\n",
    "noise = 0.2\n",
    "X_train, y_train = create_toy_data(\n",
    "    func=func, interval=interval, sample_num=train_num, noise=noise\n",
    ")\n",
    "X_test, y_test = create_toy_data(\n",
    "    func=func, interval=interval, sample_num=test_num, noise=noise\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss descend from 0.5177091128552223 to 0.265162756220404\n",
      "loss descend from 0.6082352925610521 to 0.2901658669296496\n",
      "loss descend from 0.3674146293210881 to 0.3010038609147349\n",
      "loss descend from 0.5912297622761741 to 0.325352638381563\n",
      "loss descend from 0.4318793326069768 to 0.341152280595461\n",
      "loss descend from 0.39613244004498116 to 0.35461583997845686\n",
      "loss descend from 0.38008992891306215 to 0.3657153009708922\n",
      "loss descend from 0.5626741432370647 to 0.3749834878366729\n",
      "loss descend from 0.328003501532229 to 0.3827970931920398\n",
      "loss descend from 0.6773891688178112 to 0.38951009080776333\n",
      "loss descend from 0.48210448666510514 to 0.395313552838333\n",
      "loss descend from 0.5015246906798914 to 0.4003904211144439\n",
      "loss descend from 0.42497450023313943 to 0.40486796519454343\n",
      "loss descend from 0.5391141930326003 to 0.4088462024187403\n",
      "loss descend from 0.7347079143574299 to 0.41240422530693177\n",
      "loss descend from 0.45984680351832413 to 0.41560512581959236\n",
      "loss descend from 0.5439520909178067 to 0.4185000453509323\n",
      "loss descend from 0.4888101323674769 to 0.4211307990343905\n",
      "loss descend from 0.42585210371356663 to 0.42353191842104443\n",
      "loss descend from 0.46967514396067095 to 0.42573217215805387\n",
      "loss descend from 0.6407501201118014 to 0.4277557206607567\n",
      "loss descend from 0.5012495829933665 to 0.4296230024908939\n",
      "loss descend from 0.5815337499600275 to 0.43135142413791494\n",
      "loss descend from 0.48766837256555584 to 0.43295590182243726\n",
      "loss descend from 0.46621515629774424 to 0.4344492912141727\n",
      "loss descend from 0.5176141299700489 to 0.43584273111812083\n",
      "loss descend from 0.49771266297227534 to 0.43714592063064217\n",
      "loss descend from 0.6623441760867574 to 0.4383673443053421\n",
      "loss descend from 0.611554669962035 to 0.4395144563622285\n",
      "loss descend from 0.5268190223266599 to 0.44059383237395083\n",
      "loss descend from 0.43652720097540715 to 0.44161129493461476\n",
      "loss descend from 0.5357120286563132 to 0.4425720183710551\n",
      "loss descend from 0.44201115048093453 to 0.4434806164623559\n",
      "loss descend from 0.6366745754146435 to 0.44434121629851087\n",
      "loss descend from 0.7663894051802299 to 0.445157520766864\n",
      "loss descend from 0.3953142626751807 to 0.44593286165723356\n",
      "loss descend from 0.38494044813595807 to 0.44667024498812313\n",
      "loss descend from 0.5707043677895799 to 0.4473723898511623\n",
      "loss descend from 0.5619092539987987 to 0.44804176182956\n",
      "loss descend from 0.5098093802316866 to 0.4486806018543405\n",
      "loss descend from 0.47767829955200475 to 0.44929095120853507\n",
      "loss descend from 0.4474245328516781 to 0.4498746732659499\n",
      "loss descend from 0.4661430526172869 to 0.45043347245124293\n",
      "loss descend from 0.5861395547920837 to 0.4509689108268811\n",
      "loss descend from 0.46068681063244254 to 0.4514824226463053\n",
      "loss descend from 0.6439591194262488 to 0.451975327158299\n",
      "loss descend from 0.5887622101387161 to 0.45244883990283685\n",
      "loss descend from 0.42947689448680093 to 0.4529040827016862\n",
      "loss descend from 0.41535714065329427 to 0.4533420925163413\n",
      "loss descend from 0.4618930118948094 to 0.45376382932027837\n",
      "loss descend from 0.5138666928128549 to 0.4541701831111222\n",
      "loss descend from 0.4853683924743576 to 0.4545619801703636\n",
      "loss descend from 0.7321963883139803 to 0.4549399886631404\n",
      "loss descend from 0.5611671623251516 to 0.45530492365782976\n",
      "loss descend from 0.46430997396297147 to 0.4556574516343697\n",
      "loss descend from 0.572958406494556 to 0.45599819454103696\n",
      "loss descend from 0.5386409819438782 to 0.4563277334515589\n",
      "loss descend from 0.5237992239480737 to 0.45664661186774785\n",
      "loss descend from 0.45715732530589126 to 0.4569553387070908\n",
      "loss descend from 0.4911556756403014 to 0.4572543910098001\n",
      "loss descend from 0.49844258848457385 to 0.4575442163955788\n",
      "loss descend from 0.4248323525234395 to 0.45782523529667923\n",
      "loss descend from 0.5440181232476864 to 0.4580978429906619\n",
      "loss descend from 0.5011492983225049 to 0.45836241145350043\n",
      "loss descend from 0.4624220941373459 to 0.4586192910512831\n",
      "loss descend from 0.3321394724645782 to 0.45886881208667596\n",
      "loss descend from 0.42497039667531916 to 0.45911128621448394\n",
      "loss descend from 0.47304389318388135 to 0.45934700773905707\n",
      "loss descend from 0.4864882195783457 to 0.4595762548048856\n",
      "loss descend from 0.4217087090934271 to 0.45979929049050344\n",
      "loss descend from 0.5194644801426666 to 0.4600163638147361\n",
      "loss descend from 0.7182909484093277 to 0.4602277106633798\n",
      "loss descend from 0.4933955933159276 to 0.4604335546435531\n",
      "loss descend from 0.42177235190669493 to 0.460634107872224\n",
      "loss descend from 0.3613068615464839 to 0.4608295717047509\n",
      "loss descend from 0.4023429159371043 to 0.46102013740869585\n",
      "loss descend from 0.6843984651090868 to 0.4612059867876444\n",
      "loss descend from 0.5568516940377393 to 0.461387292759307\n",
      "loss descend from 0.3731334477913062 to 0.46156421989176366\n",
      "loss descend from 0.44139354191068253 to 0.46173692490134344\n",
      "loss descend from 0.38510722713301876 to 0.4619055571153045\n",
      "loss descend from 0.7048998491806703 to 0.4620702589021816\n",
      "loss descend from 0.460322778823561 to 0.4622311660724093\n",
      "loss descend from 0.4268816313132787 to 0.46238840825158545\n",
      "loss descend from 0.4037290414688732 to 0.462542109228532\n",
      "loss descend from 0.40171918081940683 to 0.46269238728011514\n",
      "loss descend from 0.5533118021939182 to 0.4628393554746164\n",
      "loss descend from 0.6931572866209558 to 0.46298312195528646\n",
      "loss descend from 0.48078861664536937 to 0.46312379020557815\n",
      "loss descend from 0.43261379716678144 to 0.46326145929742235\n",
      "loss descend from 0.6569672870149155 to 0.46339622412380155\n",
      "loss descend from 0.435511741749455 to 0.4635281756167622\n",
      "loss descend from 0.5548377570685916 to 0.46365740095192387\n",
      "loss descend from 0.5360627330293978 to 0.4637839837404479\n",
      "loss descend from 0.40927695220720256 to 0.46390800420935285\n",
      "loss descend from 0.5317470268436706 to 0.46402953937099656\n",
      "loss descend from 0.41070805758797524 to 0.46414866318247355\n",
      "loss descend from 0.4487373993182962 to 0.4642654466956228\n",
      "loss descend from 0.7060525706584856 to 0.46437995819828415\n",
      "loss descend from 0.41176404827683655 to 0.46449226334739385\n",
      "loss descend from 0.40679532348725217 to 0.464602425294462\n",
      "loss descend from 0.4854545194122427 to 0.4647105048039389\n",
      "loss descend from 0.48917692994313017 to 0.4648165603649333\n",
      "loss descend from 0.5079232009644252 to 0.4649206482967153\n",
      "loss descend from 0.44880926138699156 to 0.46502282284840374\n",
      "loss descend from 0.5327424060368137 to 0.4651231362932079\n",
      "loss descend from 0.6791375600767284 to 0.4652216390175678\n",
      "loss descend from 0.44329808767787976 to 0.465318379605513\n",
      "loss descend from 0.6452715733735953 to 0.46541340491853483\n",
      "loss descend from 0.4982315467974772 to 0.4655067601712494\n",
      "loss descend from 0.37260612568199647 to 0.4655984890031085\n",
      "loss descend from 0.5125008553014664 to 0.46568863354639345\n",
      "loss descend from 0.4525861666879292 to 0.4657772344907204\n",
      "loss descend from 0.5183597123409098 to 0.4658643311442591\n",
      "loss descend from 0.42356325846770043 to 0.46594996149186135\n",
      "loss descend from 0.5096977628324265 to 0.4660341622502792\n",
      "loss descend from 0.5764120333971213 to 0.46611696892064164\n",
      "loss descend from 0.4897611085925383 to 0.46619841583834565\n",
      "loss descend from 0.40756895229487333 to 0.46627853622051224\n",
      "loss descend from 0.5010631440864747 to 0.4663573622111414\n",
      "loss descend from 0.46498174026602274 to 0.46643492492409755\n",
      "loss descend from 0.3465914486333181 to 0.46651125448404557\n",
      "loss descend from 0.4180685247805683 to 0.46658638006544967\n",
      "loss descend from 0.5131630959264896 to 0.46666032992974216\n",
      "loss descend from 0.6458616208947253 to 0.46673313146076095\n",
      "loss descend from 0.36313632601072604 to 0.4668048111985496\n",
      "loss descend from 0.4413889633730209 to 0.4668753948716071\n",
      "loss descend from 0.5310217628458294 to 0.46694490742767003\n",
      "loss descend from 0.49348682982556874 to 0.4670133730631042\n",
      "loss descend from 0.4258101938968765 to 0.4670808152509783\n",
      "loss descend from 0.5292198442733003 to 0.4671472567678883\n",
      "loss descend from 0.4351666626232273 to 0.4672127197195979\n",
      "loss descend from 0.6605734502801293 to 0.4672772255655528\n",
      "loss descend from 0.4947346131496982 to 0.46734079514232946\n",
      "loss descend from 0.3884682491604104 to 0.46740344868606976\n",
      "loss descend from 0.47159203105214786 to 0.4674652058539526\n",
      "loss descend from 0.4927959455546747 to 0.4675260857447517\n",
      "loss descend from 0.48860129356740617 to 0.46758610691852215\n",
      "loss descend from 0.42368563574078594 to 0.46764528741546224\n",
      "loss descend from 0.46640739621882693 to 0.4677036447739855\n",
      "loss descend from 0.5154934787143135 to 0.4677611960480466\n",
      "loss descend from 0.7276222701071297 to 0.4678179578237534\n",
      "loss descend from 0.5091534954602839 to 0.46787394623529904\n",
      "loss descend from 0.46816188349922727 to 0.4679291769802499\n",
      "loss descend from 0.5857488224126286 to 0.4679836653342144\n",
      "loss descend from 0.6447893693433915 to 0.46803742616492655\n",
      "loss descend from 0.522011360663281 to 0.46809047394576775\n",
      "loss descend from 0.4130186770388884 to 0.46814282276875513\n",
      "loss descend from 0.521904495922767 to 0.46819448635701977\n",
      "loss descend from 0.3994885175522331 to 0.4682454780767976\n",
      "loss descend from 0.41536338337509815 to 0.4682958109489564\n",
      "loss descend from 0.49131318864479895 to 0.4683454976600784\n",
      "loss descend from 0.527976270074006 to 0.468394550573119\n",
      "loss descend from 0.529322853904231 to 0.46844298173766064\n",
      "loss descend from 0.35974955458141217 to 0.46849080289977846\n",
      "loss descend from 0.4970007058365631 to 0.4685380255115367\n",
      "loss descend from 0.5932031536538163 to 0.46858466074012944\n",
      "loss descend from 0.6788472726979116 to 0.46863071947668383\n",
      "loss descend from 0.455207616625974 to 0.4686762123447377\n",
      "loss descend from 0.7131189268386194 to 0.46872114970840756\n",
      "loss descend from 0.5262867225880338 to 0.4687655416802588\n",
      "loss descend from 0.49023707564235036 to 0.468809398128891\n",
      "loss descend from 0.3913990956007935 to 0.4688527286862509\n",
      "loss descend from 0.5420223830379439 to 0.46889554275468365\n",
      "loss descend from 0.5349561999553268 to 0.46893784951373285\n",
      "loss descend from 0.46926836913542747 to 0.4689796579267023\n",
      "loss descend from 0.6086215468754984 to 0.4690209767469838\n",
      "loss descend from 0.6378970623495062 to 0.4690618145241675\n",
      "loss descend from 0.6433152795022914 to 0.46910217960993833\n",
      "loss descend from 0.3977728705108042 to 0.4691420801637756\n",
      "loss descend from 0.4270855300286754 to 0.46918152415842584\n",
      "loss descend from 0.5667111064039217 to 0.4692205193852007\n",
      "loss descend from 0.5932179532327074 to 0.46925907345911466\n",
      "loss descend from 0.7108102828981359 to 0.4692971938228351\n",
      "loss descend from 0.4770637647288202 to 0.46933488775638627\n",
      "loss descend from 0.4668234284425096 to 0.4693721623754156\n",
      "loss descend from 0.38490255927727407 to 0.469409024670229\n",
      "loss descend from 0.6730914762507914 to 0.46944548117616314\n",
      "loss descend from 0.786371772892904 to 0.4694815381436205\n",
      "loss descend from 0.44430521447234667 to 0.4695172050982352\n",
      "loss descend from 0.4375898228406618 to 0.4695524866341552\n",
      "loss descend from 0.5069741077048256 to 0.4695873733708442\n",
      "loss descend from 0.48596229755400755 to 0.4696219277463667\n",
      "loss descend from 0.57799241799489 to 0.46965588851766066\n",
      "loss descend from 0.6339013608611455 to 0.46968921731458907\n",
      "loss descend from 0.4290492250046442 to 0.46972390878433734\n",
      "loss descend from 0.5342996870685847 to 0.46975612925646265\n",
      "loss descend from 0.5196985850633083 to 0.4697863582990932\n",
      "loss descend from 0.42425979283755155 to 0.4698517913712614\n",
      "loss descend from 0.6332209660263053 to 0.46974305754879886\n",
      "loss descend from 0.6193250736781882 to 0.46946603084108207\n",
      "loss descend from 0.467378666966921 to 0.4702450119051541\n",
      "loss descend from 0.5539102629898841 to 0.4675373346386573\n",
      "loss descend from 0.3988723767687588 to 0.47363759210158135\n",
      "loss descend from 0.4025796319851338 to 0.4821622248356848\n",
      "loss descend from 0.5737139581636647 to 0.44220057226713266\n",
      "loss descend from 0.6797105398139566 to 0.3283118906342341\n",
      "loss descend from 0.43741579728741414 to 0.9701873504081238\n",
      "loss descend from 0.4395858716987858 to 0.8965948922019699\n",
      "loss descend from 0.39122400190015866 to 26.630299961455318\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "degree=50\n",
    "alphatop=200\n",
    "leng=200\n",
    "poly=pf(degree=degree)\n",
    "x_train_poly=poly.fit_transform(X_train)\n",
    "X_test_poly=poly.fit_transform(X_test)\n",
    "alpha=np.linspace(0,alphatop,leng)\n",
    "mse_total_r = np.linspace(0, 10, leng)\n",
    "mse_train_r = np.linspace(0, 10, leng)\n",
    "mse_test_r = np.linspace(0, 10, leng)\n",
    "for i in range(leng):\n",
    "    lambda_reg=alpha[i]\n",
    "    model=linearmodel(degree+1,1)\n",
    "    optimize=L2SGD(model=model,lambda_reg=lambda_reg)\n",
    "    loss_fn=MSE(model=model)\n",
    "    fit(model=model,optimize=optimize,loss_fn=loss_fn,X_train=x_train_poly,y_train=y_train)\n",
    "    y_pred_train=model(x_train_poly)\n",
    "    m_train=loss_fn(input=y_pred_train,target=y_train)\n",
    "    y_pred_test=model(X_test_poly)\n",
    "    m_test=loss_fn(input=y_pred_test,target=y_test)\n",
    "    mse_total_r[i]=m_test+m_train\n",
    "    mse_train_r[i]=m_train\n",
    "    mse_test_r[i]=m_test\n",
    "    \n",
    "plt.rc(\"font\", family=\"times new roman\")\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(alpha[:100], mse_total_r[:100], label=\"mse_total_r\")\n",
    "plt.plot(alpha[:100], mse_test_r[:100], label=\"mse_test_r\")\n",
    "plt.plot(alpha[:100], mse_train_r[:100], label=\"mse_train_r\")\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(alpha[100:], mse_total_r[100:], label=\"mse_total_r\")\n",
    "plt.plot(alpha[100:], mse_test_r[100:], label=\"mse_test_r\")\n",
    "plt.plot(alpha[100:], mse_train_r[100:], label=\"mse_train_r\")\n",
    "plt.legend(fontsize=\"x-large\")\n",
    "plt.ylabel(\"errors\")\n",
    "plt.xlabel(\"lambda\")\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As the logs show, the loss_end is smaller than the loss_start as is expected initially, as the lambda_reg increases the loss_end becomes much larger than less_begin. This is because the loss did not take regularization into account, however the optimization did. Hence the abonormal increase. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As the figure shows, with the lambda_reg increasing, the total_mse is slightly dropping at first, and finnally rocketing sky high. This indicates the process from underregularization to overregularization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
